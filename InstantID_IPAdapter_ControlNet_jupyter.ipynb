{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexmaks/n8n/blob/master/InstantID_IPAdapter_ControlNet_jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone -b totoro https://github.com/camenduru/ComfyUI /content/TotoroUI || true\n",
        "!git clone -b totoro_v2 https://github.com/camenduru/ComfyUI_IPAdapter_plus /content/TotoroUI/IPAdapter || true\n",
        "!git clone -b totoro https://github.com/camenduru/ComfyUI_InstantID /content/TotoroUI/InstantID || true\n",
        "\n",
        "# без torchtext/torchdata — они тут не нужны\n",
        "!pip install -q torch==2.2.1+cu121 torchvision==0.17.1+cu121 torchaudio==2.2.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.25 insightface onnxruntime onnxruntime-gpu\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://civitai.com/api/download/models/470847 -d /content/TotoroUI/models -o raemuXL_v35Lightning.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/h94/IP-Adapter/resolve/main/models/image_encoder/model.safetensors -d /content/TotoroUI/models/clip_vision -o CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus-face_sdxl_vit-h.safetensors  -d /content/TotoroUI/models/ipadapter -o ip-adapter-plus-face_sdxl_vit-h.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/thibaud_xl_openpose.safetensors -d /content/TotoroUI/models/controlnet -o thibaud_xl_openpose.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://github.com/Ttl/ComfyUi_NNLatentUpscale/raw/master/sdxl_resizer.pt -d /content/TotoroUI/models -o sdxl_resizer.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/DIAMONIK7777/antelopev2/resolve/main/1k3d68.onnx -d /content/TotoroUI/models/insightface/models/antelopev2 -o 1k3d68.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/DIAMONIK7777/antelopev2/resolve/main/2d106det.onnx -d /content/TotoroUI/models/insightface/models/antelopev2 -o 2d106det.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/DIAMONIK7777/antelopev2/resolve/main/genderage.onnx -d /content/TotoroUI/models/insightface/models/antelopev2 -o genderage.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/DIAMONIK7777/antelopev2/resolve/main/glintr100.onnx -d /content/TotoroUI/models/insightface/models/antelopev2 -o glintr100.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/DIAMONIK7777/antelopev2/resolve/main/scrfd_10g_bnkps.onnx -d /content/TotoroUI/models/insightface/models/antelopev2 -o scrfd_10g_bnkps.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/InstantX/InstantID/resolve/main/ip-adapter.bin -d /content/TotoroUI/models/instantid -o ip-adapter.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/InstantX/InstantID/resolve/main/ControlNetModel/diffusion_pytorch_model.safetensors -d /content/TotoroUI/models/controlnet/SDXL/instantid -o diffusion_pytorch_model.safetensors\n",
        "\n",
        "!wget -q https://huggingface.co/camenduru/IICF/resolve/main/test/anya.jpg -O /content/anya.jpg\n",
        "!wget -q https://huggingface.co/camenduru/IICF/resolve/main/test/pose_images.zip -O /content/pose_images.zip\n",
        "!unzip -o /content/pose_images.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfRqlj6A3vOX",
        "outputId": "5134d61a-0ccc-4502-ce09-c787b2ec9115"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path '/content/TotoroUI' already exists and is not an empty directory.\n",
            "fatal: destination path '/content/TotoroUI/IPAdapter' already exists and is not an empty directory.\n",
            "fatal: destination path '/content/TotoroUI/InstantID' already exists and is not an empty directory.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m854.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25haria2 is already the newest version (1.36.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "50f141|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/TotoroUI/models/raemuXL_v35Lightning.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "b5033e|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/TotoroUI/models/clip_vision/CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "fdf6dc|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/TotoroUI/models/ipadapter/ip-adapter-plus-face_sdxl_vit-h.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "6ce53e|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/TotoroUI/models/controlnet/thibaud_xl_openpose.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "8f2d8f|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/TotoroUI/models/sdxl_resizer.pt\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "82a70e|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/TotoroUI/models/insightface/models/antelopev2/1k3d68.onnx\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "f6c4d9|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/TotoroUI/models/insightface/models/antelopev2/2d106det.onnx\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "0436dc|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/TotoroUI/models/insightface/models/antelopev2/genderage.onnx\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "b446b8|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/TotoroUI/models/insightface/models/antelopev2/glintr100.onnx\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "78f5df|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/TotoroUI/models/insightface/models/antelopev2/scrfd_10g_bnkps.onnx\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "6fae3c|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/TotoroUI/models/instantid/ip-adapter.bin\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "d3cf74|\u001b[1;32mOK\u001b[0m  |       0B/s|/content/TotoroUI/models/controlnet/SDXL/instantid/diffusion_pytorch_model.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "Archive:  /content/pose_images.zip\n",
            "  inflating: pose_images/headshot_dw_pose_00001_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00002_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00003_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00004_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00005_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00006_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00007_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00008_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00009_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00010_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00011_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00012_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00013_.png  \n",
            "  inflating: pose_images/headshot_dw_pose_00014_.png  \n",
            "  inflating: pose_images/headshot_kps_00001_.png  \n",
            "  inflating: pose_images/headshot_kps_00002_.png  \n",
            "  inflating: pose_images/headshot_kps_00003_.png  \n",
            "  inflating: pose_images/headshot_kps_00004_.png  \n",
            "  inflating: pose_images/headshot_kps_00005_.png  \n",
            "  inflating: pose_images/headshot_kps_00006_.png  \n",
            "  inflating: pose_images/headshot_kps_00007_.png  \n",
            "  inflating: pose_images/headshot_kps_00008_.png  \n",
            "  inflating: pose_images/headshot_kps_00009_.png  \n",
            "  inflating: pose_images/headshot_kps_00010_.png  \n",
            "  inflating: pose_images/headshot_kps_00011_.png  \n",
            "  inflating: pose_images/headshot_kps_00012_.png  \n",
            "  inflating: pose_images/headshot_kps_00013_.png  \n",
            "  inflating: pose_images/headshot_kps_00014_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00001_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00002_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00003_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00004_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00005_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00006_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00007_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00008_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00009_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00010_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00011_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00012_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00013_.png  \n",
            "  inflating: pose_images/headshot_open_pose_00014_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00001_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00002_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00003_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00004_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00005_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00006_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00007_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00008_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00009_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00010_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00011_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00012_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00013_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00014_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00015_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00016_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00017_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00018_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00019_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00020_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00021_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00022_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00023_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00024_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00025_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00026_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00027_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00028_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00029_.png  \n",
            "  inflating: pose_images/pose_dw_pose_00030_.png  \n",
            "  inflating: pose_images/pose_kps_00001_.png  \n",
            "  inflating: pose_images/pose_kps_00002_.png  \n",
            "  inflating: pose_images/pose_kps_00003_.png  \n",
            "  inflating: pose_images/pose_kps_00004_.png  \n",
            "  inflating: pose_images/pose_kps_00005_.png  \n",
            "  inflating: pose_images/pose_kps_00006_.png  \n",
            "  inflating: pose_images/pose_kps_00007_.png  \n",
            "  inflating: pose_images/pose_kps_00008_.png  \n",
            "  inflating: pose_images/pose_kps_00009_.png  \n",
            "  inflating: pose_images/pose_kps_00010_.png  \n",
            "  inflating: pose_images/pose_kps_00011_.png  \n",
            "  inflating: pose_images/pose_kps_00012_.png  \n",
            "  inflating: pose_images/pose_kps_00013_.png  \n",
            "  inflating: pose_images/pose_kps_00014_.png  \n",
            "  inflating: pose_images/pose_kps_00015_.png  \n",
            "  inflating: pose_images/pose_kps_00016_.png  \n",
            "  inflating: pose_images/pose_kps_00017_.png  \n",
            "  inflating: pose_images/pose_kps_00018_.png  \n",
            "  inflating: pose_images/pose_kps_00019_.png  \n",
            "  inflating: pose_images/pose_kps_00020_.png  \n",
            "  inflating: pose_images/pose_kps_00021_.png  \n",
            "  inflating: pose_images/pose_kps_00022_.png  \n",
            "  inflating: pose_images/pose_kps_00023_.png  \n",
            "  inflating: pose_images/pose_kps_00024_.png  \n",
            "  inflating: pose_images/pose_kps_00025_.png  \n",
            "  inflating: pose_images/pose_kps_00026_.png  \n",
            "  inflating: pose_images/pose_kps_00027_.png  \n",
            "  inflating: pose_images/pose_kps_00028_.png  \n",
            "  inflating: pose_images/pose_kps_00029_.png  \n",
            "  inflating: pose_images/pose_kps_00030_.png  \n",
            "  inflating: pose_images/pose_open_pose_00001_.png  \n",
            "  inflating: pose_images/pose_open_pose_00002_.png  \n",
            "  inflating: pose_images/pose_open_pose_00003_.png  \n",
            "  inflating: pose_images/pose_open_pose_00004_.png  \n",
            "  inflating: pose_images/pose_open_pose_00005_.png  \n",
            "  inflating: pose_images/pose_open_pose_00006_.png  \n",
            "  inflating: pose_images/pose_open_pose_00007_.png  \n",
            "  inflating: pose_images/pose_open_pose_00008_.png  \n",
            "  inflating: pose_images/pose_open_pose_00009_.png  \n",
            "  inflating: pose_images/pose_open_pose_00010_.png  \n",
            "  inflating: pose_images/pose_open_pose_00011_.png  \n",
            "  inflating: pose_images/pose_open_pose_00012_.png  \n",
            "  inflating: pose_images/pose_open_pose_00013_.png  \n",
            "  inflating: pose_images/pose_open_pose_00014_.png  \n",
            "  inflating: pose_images/pose_open_pose_00015_.png  \n",
            "  inflating: pose_images/pose_open_pose_00016_.png  \n",
            "  inflating: pose_images/pose_open_pose_00017_.png  \n",
            "  inflating: pose_images/pose_open_pose_00018_.png  \n",
            "  inflating: pose_images/pose_open_pose_00019_.png  \n",
            "  inflating: pose_images/pose_open_pose_00020_.png  \n",
            "  inflating: pose_images/pose_open_pose_00021_.png  \n",
            "  inflating: pose_images/pose_open_pose_00022_.png  \n",
            "  inflating: pose_images/pose_open_pose_00023_.png  \n",
            "  inflating: pose_images/pose_open_pose_00024_.png  \n",
            "  inflating: pose_images/pose_open_pose_00025_.png  \n",
            "  inflating: pose_images/pose_open_pose_00026_.png  \n",
            "  inflating: pose_images/pose_open_pose_00027_.png  \n",
            "  inflating: pose_images/pose_open_pose_00028_.png  \n",
            "  inflating: pose_images/pose_open_pose_00029_.png  \n",
            "  inflating: pose_images/pose_open_pose_00030_.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b totoro https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "!git clone -b totoro_v2 https://github.com/camenduru/ComfyUI_IPAdapter_plus /content/TotoroUI/IPAdapter\n",
        "!git clone -b totoro https://github.com/camenduru/ComfyUI_InstantID /content/TotoroUI/InstantID\n",
        "\n",
        "!pip install -q torch==2.2.1+cu121 torchvision==0.17.1+cu121 torchaudio==2.2.1+cu121 torchtext==0.17.1 torchdata==0.7.1 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.25 insightface onnxruntime onnxruntime-gpu\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://civitai.com/api/download/models/470847 -d /content/TotoroUI/models -o raemuXL_v35Lightning.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/h94/IP-Adapter/resolve/main/models/image_encoder/model.safetensors -d /content/TotoroUI/models/clip_vision -o CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus-face_sdxl_vit-h.safetensors  -d /content/TotoroUI/models/ipadapter -o ip-adapter-plus-face_sdxl_vit-h.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/thibaud_xl_openpose.safetensors -d /content/TotoroUI/models/controlnet -o thibaud_xl_openpose.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://github.com/Ttl/ComfyUi_NNLatentUpscale/raw/master/sdxl_resizer.pt -d /content/TotoroUI/models -o sdxl_resizer.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/DIAMONIK7777/antelopev2/resolve/main/1k3d68.onnx -d /content/TotoroUI/models/insightface/models/antelopev2 -o 1k3d68.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/DIAMONIK7777/antelopev2/resolve/main/2d106det.onnx -d /content/TotoroUI/models/insightface/models/antelopev2 -o 2d106det.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/DIAMONIK7777/antelopev2/resolve/main/genderage.onnx -d /content/TotoroUI/models/insightface/models/antelopev2 -o genderage.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/DIAMONIK7777/antelopev2/resolve/main/glintr100.onnx -d /content/TotoroUI/models/insightface/models/antelopev2 -o glintr100.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/DIAMONIK7777/antelopev2/resolve/main/scrfd_10g_bnkps.onnx -d /content/TotoroUI/models/insightface/models/antelopev2 -o scrfd_10g_bnkps.onnx\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/InstantX/InstantID/resolve/main/ip-adapter.bin -d /content/TotoroUI/models/instantid -o ip-adapter.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/InstantX/InstantID/resolve/main/ControlNetModel/diffusion_pytorch_model.safetensors -d /content/TotoroUI/models/controlnet/SDXL/instantid -o diffusion_pytorch_model.safetensors\n",
        "\n",
        "!wget https://huggingface.co/camenduru/IICF/resolve/main/test/anya.jpg -O /content/anya.jpg\n",
        "!wget https://huggingface.co/camenduru/IICF/resolve/main/test/pose_images.zip -O /content/pose_images.zip\n",
        "!unzip /content/pose_images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwtVuUsd3Mca"
      },
      "outputs": [],
      "source": [
        "%cd /content/TotoroUI\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import totoro\n",
        "import nodes\n",
        "import sys\n",
        "sys.path.append('/content/TotoroUI/IPAdapter')\n",
        "import IPAdapterPlus\n",
        "sys.path.append('/content/TotoroUI/InstantID')\n",
        "import InstantID\n",
        "import scipy\n",
        "import model_management\n",
        "from latent_resizer import LatentResizer\n",
        "from totoro import model_management\n",
        "import gc\n",
        "import random\n",
        "\n",
        "def upscale(latent, upscale):\n",
        "  device = model_management.get_torch_device()\n",
        "  samples = latent.to(device=device, dtype=torch.float16)\n",
        "  model = LatentResizer.load_model('/content/TotoroUI/models/sdxl_resizer.pt', device, torch.float16)\n",
        "  model.to(device=device)\n",
        "  latent_out = (model(0.13025 * samples, scale=upscale) / 0.13025)\n",
        "  latent_out = latent_out.to(device=\"cpu\")\n",
        "  model.to(device=model_management.vae_offload_device())\n",
        "  return ({\"samples\": latent_out},)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model_patcher, clip, vae, clipvision = totoro.sd.load_checkpoint_guess_config(\"/content/TotoroUI/models/raemuXL_v35Lightning.safetensors\", output_vae=True, output_clip=True, embedding_directory=None)\n",
        "    IPAdapterPlus_model = IPAdapterPlus.IPAdapterUnifiedLoader().load_models(model_patcher, 'PLUS FACE (portraits)', lora_strength=0.0, provider=\"CUDA\")\n",
        "    instantid = InstantID.InstantIDModelLoader().load_model(\"/content/TotoroUI/models/instantid/ip-adapter.bin\")\n",
        "    insightface = InstantID.InstantIDFaceAnalysis().load_insight_face(\"CUDA\")\n",
        "    instantid_control_net = totoro.controlnet.load_controlnet(\"/content/TotoroUI/models/controlnet/SDXL/instantid/diffusion_pytorch_model.safetensors\")\n",
        "    output_image, output_mask = nodes.LoadImage().load_image(\"/content/anya.jpg\")\n",
        "    image_kps, image_kps_mask = nodes.LoadImage().load_image(\"/content/pose_images/pose_kps_00008_.png\")\n",
        "    image_dw, image_dw_mask = nodes.LoadImage().load_image(\"/content/pose_images/pose_dw_pose_00008_.png\")\n",
        "    ip_model_patcher = IPAdapterPlus.IPAdapterAdvanced().apply_ipadapter(IPAdapterPlus_model[0], IPAdapterPlus_model[1], image=output_image, weight_type=\"style transfer\")\n",
        "    tokens = clip.tokenize(\"1girl\")\n",
        "    cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)\n",
        "    cond = [[cond, {\"pooled_output\": pooled}]]\n",
        "    n_tokens = clip.tokenize(\"(nsfw:1.5), nipple, nude, naked, lowres, child, getty, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, trademark, watermark, title, multiple view, reference sheet, mutated hands and fingers, poorly drawn face, mutation, deformed, ugly, bad proportions, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, tatoo, amateur drawing, odd eyes, uneven eyes, unnatural face, uneven nostrils, crooked mouth, bad teeth, crooked teeth, photoshop, video game, censor, censored, ghost, b&w, weird colors, gradient background, spotty background, blurry background, ugly background, simple background, realistic, out of frame, extra objects, gross, ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of focus, blurry, very long body, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn eyes, cloned face, disfigured, deformed, cross-eye, extra limbs, missing limb, malformed hands, mutated, morbid, mutilated, disfigured, extra arms, extra hands, mangled fingers, contorted, conjoined, mismatched limbs, mismatched parts, bad perspective, black and white, oversaturated, undersaturated, bad shadow, cropped image, draft, grainy, pixelated\")\n",
        "    n_cond, n_pooled = clip.encode_from_tokens(n_tokens, return_pooled=True)\n",
        "    n_cond = [[n_cond, {\"pooled_output\": n_pooled}]]\n",
        "    work_model, instantid_cond, instantid_n_cond = InstantID.ApplyInstantID().apply_instantid(instantid=instantid[0], insightface=insightface[0], control_net=instantid_control_net, image=output_image, model=ip_model_patcher[0], positive=cond, negative=n_cond, start_at=0.0, end_at=1.0, weight=0.80, image_kps=image_kps)\n",
        "    openpose_control_net = totoro.controlnet.load_controlnet(\"/content/TotoroUI/models/controlnet/thibaud_xl_openpose.safetensors\")\n",
        "    openpose_cond = nodes.ControlNetApply().apply_controlnet(conditioning=instantid_cond, control_net=openpose_control_net, image=image_dw, strength=0.90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "FS_zGvPC3Mcb",
        "outputId": "ef5f4312-2924-4105-b5c9-7798475bac1a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_management' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2540827002.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mclipvision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mIPAdapterPlus_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_empty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_management' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# новый сид на каждый прогон\n",
        "ran = random.randint(0, 65535)\n",
        "print(\"seed:\", ran)\n",
        "\n",
        "with torch.no_grad():\n",
        "    latent = {\n",
        "        \"samples\": torch.zeros(\n",
        "            [1, 4, 1024 // 8, 1024 // 8],\n",
        "            dtype=torch.float16,\n",
        "        )\n",
        "    }\n",
        "\n",
        "    sample = nodes.common_ksampler(\n",
        "        model=work_model,\n",
        "        seed=ran,\n",
        "        steps=4,\n",
        "        cfg=1.3,\n",
        "        sampler_name=\"dpmpp_sde_gpu\",\n",
        "        scheduler=\"karras\",\n",
        "        positive=openpose_cond[0],\n",
        "        negative=instantid_n_cond,\n",
        "        latent=latent,\n",
        "        denoise=0.95,\n",
        "    )\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        samples = sample[0][\"samples\"].to(torch.float16)\n",
        "        vae.first_stage_model.cuda()\n",
        "        decoded = vae.decode_tiled(samples).detach().cpu()\n",
        "\n",
        "img = Image.fromarray(\n",
        "    np.array(decoded.clamp(0, 1) * 255, dtype=np.uint8)[0]\n",
        ")\n",
        "display(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tth1e8P93Mcb"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  latent = upscale(sample, 1.5)\n",
        "  sample = nodes.common_ksampler(model=work_model,\n",
        "                            seed=ran,\n",
        "                            steps=4,\n",
        "                            cfg=1.3,\n",
        "                            sampler_name=\"dpmpp_sde_gpu\",\n",
        "                            scheduler=\"karras\",\n",
        "                            positive=openpose_cond[0],\n",
        "                            negative=instantid_n_cond,\n",
        "                            latent=latent[0],\n",
        "                            denoise=0.55)\n",
        "  with torch.inference_mode():\n",
        "    sample = sample[0][\"samples\"].to(torch.float16)\n",
        "    vae.first_stage_model.cuda()\n",
        "    decoded = vae.decode_tiled(sample).detach()\n",
        "Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}